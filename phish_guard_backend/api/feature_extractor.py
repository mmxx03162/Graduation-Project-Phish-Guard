"""
ğŸ” Feature Extractor - ÙŠØ·Ø§Ø¨Ù‚ Ø£Ø¹Ù…Ø¯Ø© Dataset Ø§Ù„ÙØ¹Ù„ÙŠØ©
====================================================
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ 9 features Ø¨Ù†ÙØ³ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù„ÙŠ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§ØªØ¯Ø±Ø¨Øª Ø¹Ù„ÙŠÙ‡Ø§
"""

import re
from urllib.parse import urlparse
import socket
import ssl
import requests
from datetime import datetime

class PhishingFeatureExtractor:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ 9 features Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ø£Ø¹Ù…Ø¯Ø© dataset Ø§Ù„Ø£ØµÙ„ÙŠ
    """
    
    def __init__(self):
        self.feature_names = [
            'URLURL_Length',
            'having_At_Symbol',
            'Prefix_Suffix',
            'having_Sub_Domain',
            'SSLfinal_State',
            'Domain_registeration_length',
            'age_of_domain',
            'DNSRecord',
            'Page_Rank'
        ]
    
    def extract_features(self, url):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ 9 features Ù…Ù† URL
        
        Returns:
            dict: {feature_name: value, ...}
        """
        features = {}
        
        try:
            parsed = urlparse(url)
            domain = parsed.netloc or parsed.path
            
            # 1. URLURL_Length (Ø·ÙˆÙ„ Ø§Ù„Ù€ URL)
            features['URLURL_Length'] = len(url)
            
            # 2. having_At_Symbol (ÙˆØ¬ÙˆØ¯ Ø±Ù…Ø² @)
            features['having_At_Symbol'] = 1 if '@' in url else -1
            
            # 3. Prefix_Suffix (ÙˆØ¬ÙˆØ¯ - ÙÙŠ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†)
            features['Prefix_Suffix'] = 1 if '-' in domain else -1
            
            # 4. having_Sub_Domain (Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· = Ø¹Ø¯Ø¯ Ø§Ù„Ù€ subdomains)
            dot_count = domain.count('.')
            if dot_count == 0:
                features['having_Sub_Domain'] = 1
            elif dot_count == 1:
                features['having_Sub_Domain'] = -1
            elif dot_count == 2:
                features['having_Sub_Domain'] = 0
            else:
                features['having_Sub_Domain'] = 1
            
            # 5. SSLfinal_State (Ø­Ø§Ù„Ø© SSL)
            features['SSLfinal_State'] = self._check_ssl(url)
            
            # 6. Domain_registeration_length (Ù…Ø¯Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - ØªÙ‚Ø¯ÙŠØ±)
            # -1: Ø·ÙˆÙŠÙ„Ø©ØŒ 0: Ù…ØªÙˆØ³Ø·Ø©ØŒ 1: Ù‚ØµÙŠØ±Ø©
            features['Domain_registeration_length'] = self._estimate_domain_registration(domain)
            
            # 7. age_of_domain (Ø¹Ù…Ø± Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† - ØªÙ‚Ø¯ÙŠØ±)
            # -1: Ù‚Ø¯ÙŠÙ…ØŒ 0: Ù…ØªÙˆØ³Ø·ØŒ 1: Ø¬Ø¯ÙŠØ¯
            features['age_of_domain'] = self._estimate_domain_age(domain)
            
            # 8. DNSRecord (ÙˆØ¬ÙˆØ¯ DNS record)
            features['DNSRecord'] = self._check_dns(domain)
            
            # 9. Page_Rank (ØªÙ‚Ø¯ÙŠØ± Ø´Ù‡Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹)
            # -1: Ù…Ø´Ù‡ÙˆØ±ØŒ 0: Ù…ØªÙˆØ³Ø·ØŒ 1: ØºÙŠØ± Ù…Ø´Ù‡ÙˆØ±
            features['Page_Rank'] = self._estimate_page_rank(domain)
            
        except Exception as e:
            print(f"âš ï¸ Error extracting features: {e}")
            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ØŒ Ù†Ø±Ø¬Ø¹ Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            for name in self.feature_names:
                if name not in features:
                    features[name] = 0
        
        return features
    
    def _check_ssl(self, url):
        """ÙØ­Øµ SSL Certificate"""
        try:
            if url.startswith('https://'):
                parsed = urlparse(url)
                domain = parsed.netloc
                
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ SSL
                context = ssl.create_default_context()
                with socket.create_connection((domain, 443), timeout=3) as sock:
                    with context.wrap_socket(sock, server_hostname=domain) as ssock:
                        cert = ssock.getpeercert()
                        # SSL Ù…ÙˆØ¬ÙˆØ¯ ÙˆØµØ§Ù„Ø­
                        return -1
            else:
                # Ù„Ø§ ÙŠÙˆØ¬Ø¯ HTTPS
                return 1
        except:
            # SSL ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ø®Ø·Ø£
            return 0
    
    def _estimate_domain_registration(self, domain):
        """ØªÙ‚Ø¯ÙŠØ± Ù…Ø¯Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†"""
        # Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø´Ù‡ÙˆØ±Ø© Ø¹Ø§Ø¯Ø© Ø¹Ù†Ø¯Ù‡Ø§ ØªØ³Ø¬ÙŠÙ„ Ø·ÙˆÙŠÙ„
        famous_domains = ['google', 'facebook', 'youtube', 'amazon', 'wikipedia', 
                         'twitter', 'instagram', 'linkedin', 'microsoft', 'apple']
        
        if any(famous in domain.lower() for famous in famous_domains):
            return -1  # ØªØ³Ø¬ÙŠÙ„ Ø·ÙˆÙŠÙ„
        
        # Ø§ÙØªØ±Ø§Ø¶ÙŠ: ØªØ³Ø¬ÙŠÙ„ Ù‚ØµÙŠØ± (Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡)
        return 1
    
    def _estimate_domain_age(self, domain):
        """ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ø± Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†"""
        # Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø´Ù‡ÙˆØ±Ø© Ø¹Ø§Ø¯Ø© Ù‚Ø¯ÙŠÙ…Ø©
        famous_domains = ['google', 'facebook', 'youtube', 'amazon', 'wikipedia',
                         'twitter', 'instagram', 'linkedin', 'microsoft', 'apple',
                         'yahoo', 'reddit', 'ebay', 'netflix', 'paypal']
        
        if any(famous in domain.lower() for famous in famous_domains):
            return -1  # Ø¯ÙˆÙ…ÙŠÙ† Ù‚Ø¯ÙŠÙ…
        
        # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø¯ÙˆÙ…ÙŠÙ† Ø¬Ø¯ÙŠØ¯
        return 1
    
    def _check_dns(self, domain):
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ DNS Record"""
        try:
            socket.gethostbyname(domain)
            return -1  # DNS Ù…ÙˆØ¬ÙˆØ¯
        except:
            return 1  # DNS ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
    
    def _estimate_page_rank(self, domain):
        """ØªÙ‚Ø¯ÙŠØ± Ø´Ù‡Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Page Rank)"""
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø´Ù‡ÙˆØ±Ø© Ø¬Ø¯Ø§Ù‹
        top_sites = ['google', 'youtube', 'facebook', 'amazon', 'wikipedia',
                    'yahoo', 'reddit', 'twitter', 'instagram', 'linkedin',
                    'netflix', 'microsoft', 'apple', 'ebay', 'cnn', 'bbc']
        
        domain_lower = domain.lower()
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø´Ù‡ÙŠØ±Ø©
        if any(site in domain_lower for site in top_sites):
            return -1  # Page Rank Ø¹Ø§Ù„ÙŠ
        
        # Ù…ÙˆØ§Ù‚Ø¹ Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ø´Ù‡Ø±Ø©
        if any(ext in domain_lower for ext in ['.edu', '.gov', '.org']):
            return 0
        
        # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ù…ÙˆÙ‚Ø¹ ØºÙŠØ± Ù…Ø´Ù‡ÙˆØ±
        return 1
    
    def get_feature_vector(self, url):
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Feature Vector Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†Ø¨Ø¤
        
        Returns:
            list: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙŠÙ… Ø¨Ù†ÙØ³ ØªØ±ØªÙŠØ¨ feature_names
        """
        features_dict = self.extract_features(url)
        return [features_dict[name] for name in self.feature_names]
    
    def get_feature_dataframe(self, url):
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ DataFrame Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†Ø¨Ø¤
        """
        import pandas as pd
        features_dict = self.extract_features(url)
        return pd.DataFrame([features_dict])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âœ… Backward-compatible function API expected by predictor.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_url(url):
    """ØªØ­Ù‚Ù‚ Ø¨Ø³ÙŠØ· Ù…Ù† ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¨Ø·."""
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        return True
    except Exception:
        return False


def extract_numerical_features(url):
    """
    Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© ØªØ¶Ù… 9 Ù…ÙŠØ²Ø§Øª Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ predictor.NUMERICAL_FEATURE_NAMES:
    [
        'UrlLength', 'HostnameLength', 'NumDots', 'UsesHTTPS', 'HasSuspiciousKeyword',
        'NumDash', 'HasAtSymbol', 'NumQueryComponents', 'DomainAgeDays'
    ]
    """
    parsed = urlparse(url)
    hostname = parsed.netloc or parsed.path

    url_length = len(url)
    hostname_length = len(hostname)
    num_dots = hostname.count('.')
    uses_https = 1 if parsed.scheme.lower() == 'https' else 0

    suspicious_keywords = [
        'login', 'verify', 'update', 'secure', 'account', 'bank', 'confirm', 'pay',
        'password', 'signin', 'support', 'help', 'billing'
    ]
    lower_url = url.lower()
    has_suspicious_kw = 1 if any(k in lower_url for k in suspicious_keywords) else 0

    num_dash = hostname.count('-')
    has_at_symbol = 1 if '@' in url else 0
    num_query_components = 0
    if parsed.query:
        # count of key=value pairs
        num_query_components = sum(1 for part in parsed.query.split('&') if part)

    # ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ø± Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† (Ø¨Ø§Ù„Ø£ÙŠØ§Ù…) Ø¨Ø´ÙƒÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø¯ÙˆÙ† whois
    famous_domains = [
        'google', 'facebook', 'youtube', 'amazon', 'wikipedia', 'yahoo', 'reddit',
        'twitter', 'instagram', 'linkedin', 'microsoft', 'apple', 'paypal', 'netflix'
    ]
    if any(fd in hostname.lower() for fd in famous_domains):
        domain_age_days = 3650  # ~10 Ø³Ù†ÙˆØ§Øª
    else:
        domain_age_days = 30  # ØªÙ‚Ø¯ÙŠØ± Ù…Ø­Ø§ÙØ¸ Ù„Ù„Ø¯ÙˆÙ…ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©/Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©

    return [
        url_length,
        hostname_length,
        num_dots,
        uses_https,
        has_suspicious_kw,
        num_dash,
        has_at_symbol,
        num_query_components,
        domain_age_days,
    ]


def transform_text_features(text, vectorizer):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…ÙŠØ²Ø§Øª TF-IDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ vectorizer Ø§Ù„Ù…Ø­Ù…Ù„."""
    try:
        processed = (text or '').lower()
        return vectorizer.transform([processed])
    except Exception as e:
        print(f"Text transform error: {e}")
        return None


# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ§Ù„ÙŠØ© ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ predictor Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ
# ÙˆÙ„ÙƒÙ† ÙŠØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡Ø§ØŒ Ù„Ø°Ø§ Ù†ÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø§Øª Ø®ÙÙŠÙØ© Ù…ØªÙˆØ§ÙÙ‚Ø©.
def extract_features_for_team1(url):
    """DataFrame Ø¨Ù…ÙŠØ²Ø§Øª Ø±Ù‚Ù…ÙŠØ© Ù„Ù„ÙØ±ÙŠÙ‚ 1."""
    import pandas as pd
    cols = [
        'UrlLength', 'HostnameLength', 'NumDots', 'UsesHTTPS', 'HasSuspiciousKeyword',
        'NumDash', 'HasAtSymbol', 'NumQueryComponents', 'DomainAgeDays'
    ]
    values = extract_numerical_features(url)
    return pd.DataFrame([values], columns=cols)


def extract_features_for_team2(url, vectorizer):
    """Ù…ØµÙÙˆÙØ© Ù…ÙŠØ²Ø§Øª Ù†ØµÙŠØ© Ù„Ù„ÙØ±ÙŠÙ‚ 2."""
    return transform_text_features(url, vectorizer)


def extract_features_for_team3(url, vectorizer, scaler):
    """Ø¯Ù…Ø¬ Ù…ÙŠØ²Ø§Øª TF-IDF + Ø±Ù‚Ù…ÙŠØ© Ø¨Ø´ÙƒÙ„ CSR Ù„Ù„ÙØ±ÙŠÙ‚ 3."""
    from scipy.sparse import hstack
    num_df = extract_features_for_team1(url)
    try:
        num_scaled = scaler.transform(num_df)
    except Exception:
        # Ø¥Ø°Ø§ ØªØ¹Ø°Ø± Ø§Ù„ØªØ­Ø¬ÙŠÙ… Ù„Ø£ÙŠ Ø³Ø¨Ø¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙŠÙ… ÙƒÙ…Ø§ Ù‡ÙŠ
        num_scaled = num_df.values
    text_features = transform_text_features(url, vectorizer)
    try:
        combined = hstack([text_features, num_scaled]).tocsr()
        return combined
    except Exception as e:
        print(f"Team3 combine error: {e}")
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§ª TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import joblib
    import pandas as pd
    
    print("="*70)
    print("ğŸ” Testing Feature Extractor with Real Model")
    print("="*70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Extractor
    extractor = PhishingFeatureExtractor()
    
    # ØªØ¬Ø±Ø¨Ø© URLs
    test_urls = [
        "https://www.google.com",
        "https://www.facebook.com",
        "http://suspicious-website-12345.tk",
        "https://secure-paypal-verify.xyz"
    ]
    
    for url in test_urls:
        print(f"\n{'â”€'*70}")
        print(f"ğŸ“ URL: {url}")
        print(f"{'â”€'*70}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Features
        features = extractor.extract_features(url)
        
        print("\nğŸ“Š Extracted Features:")
        for name, value in features.items():
            symbol = "âœ…" if value == -1 else ("âš ï¸" if value == 0 else "âŒ")
            print(f"   {symbol} {name:<30}: {value:>3}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ DataFrame
        df = extractor.get_feature_dataframe(url)
        print(f"\nğŸ“‹ DataFrame shape: {df.shape}")
        print(f"   Columns: {list(df.columns)}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤
        try:
            model_data = joblib.load('ml_model/new_model_1_1_rf.joblib')
            model = model_data['model']
            
            print(f"\nğŸ¤– Model Feature Names:")
            for i, name in enumerate(model_data['feature_names'], 1):
                print(f"   {i}. {name}")
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction = model.predict(df)[0]
            proba = model.predict_proba(df)[0]
            
            result = "âœ… Legitimate" if prediction <= 0 else "ğŸš¨ Phishing"
            confidence = max(proba) * 100
            
            print(f"\nğŸ¯ Prediction: {result}")
            print(f"ğŸ“Š Confidence: {confidence:.1f}%")
            
        except FileNotFoundError:
            print(f"\nâš ï¸ Model file not found. Train the model first!")
        except Exception as e:
            print(f"\nâŒ Error during prediction: {e}")
    
    print(f"\n{'='*70}\n")